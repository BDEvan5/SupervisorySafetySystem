---
title: "Paper Data Presetation"
author: "Benjamin Evans"
date: "29/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

test_data = read.csv("PaperTable.csv")
library(ggplot2)
library(dplyr)
library(tidyr)

```

# Kernel Safety Tests


# Tests


## Kernel Space Discretisation

```{r}
test_data %>%
  filter(EvalName=="KernelDiscret_ndx")%>%
  filter(success_rate>80)%>%
  select(avg_times, success_rate, test_number, n_dx, constant_value, kernel_mode, kernel_filled)%>%
  ggplot(aes(x=n_dx, y=kernel_filled, group=kernel_mode))+
  geom_line(aes(linetype=kernel_mode))+
  geom_point()

```

The test shows that a higher percentage of the image region is filled for the discriminating kernel than the viability kernel.
It also shows that the two amounts converge as the discretization step gets smaller.

<!-- ## Kernel Time Discretization -->

## Kernel Safety

Make a table here like the one that I am going to put into the report.

# Learning Formulation

## Learning Mode

```{r}
test_data %>%
  filter(EvalName=="LearningMode") %>%
  # filter(avg_times!=0) %>%
  select(avg_times, success_rate, kernel_reward, learning, learning_mode, train_n) %>%
  group_by(learning_mode)%>%
  mutate(mean_time=mean(avg_times))%>%
  ggplot(aes(x=learning_mode, y=avg_times, group=learning_mode))+
  geom_boxplot()+
  geom_point()+
  geom_point(aes(y=mean_time), shape='x', size=10)

```

The test shows that the standard learning formulation used in robotics (episodic learning with a constant punishment) is very ineffective.
The method presented in this paper enables continuous learning where the agent is able to continue driving around the race track because it never crashes.
Continuous learning is shown to produce significantly lower lap times than episodic learning.

Additionally, due to the supervisors intervention, the severity of how bad an action is can be easily measured.
The magnitude based reward signal punishes the agent relative to the magnitude of the intervention required to keep the vehicle from crashing.
The results indicate that using a magnitude based punishment in both continuous and episodic learning leads to an improvement.

Overall, our novel learning style of continuous learning using a punishment related to the magnitude of the supervisors intervention, leads to significantly faster racing performance.

## Kernel Reward

```{r}
test_data %>%
  filter(EvalName=="KernelReward")%>%
  select(avg_times, success_rate, test_number, reward, kernel_reward, sss_reward_scale, test_number) %>%
  unite("label", kernel_reward, sss_reward_scale, sep="__")%>%
  group_by(label) %>%
  mutate(mean_success=mean(success_rate))%>%
  ggplot(aes(x=label, y=avg_times))+
  geom_boxplot(outlier.shape=NA)+
  geom_point(aes(y=avg_times), size=2)+
  geom_point(aes(y=mean_success), shape='x', size=12)
```


# Baseline Comparision


## Repeatability

We evaluate 10 agents of the baseline and the SSS.

```{r}

test_data %>%
  filter(EvalName=="BaselineComp")%>%
  filter(avg_times!=0&avg_times<200) %>%
  select(avg_times, success_rate, test_number, reward, kernel_reward, vehicle, test_number) %>%
  ggplot(aes(x=vehicle, y=(avg_times), color=test_number))+
  geom_boxplot()+
  geom_point()

```

The tests show that the SSS produces much more similar results than the baseline agent.
The standard deviation of the data is much smaller at ... vs ...

The average times produced by the SSS is also lower, showing that the SSS leads to faster racing behaviour.

## Training Rate

```{r}
test_data %>%
  filter(EvalName=="StepsSSS") %>%
  filter(avg_times!=0) %>%
  select(avg_times, success_rate, kernel_reward, vehicle, train_n) %>%
  ggplot(aes(x=train_n, y=avg_times))+
  geom_point()+
  geom_smooth(method=lm)

```
```{r}

test_data %>%
  filter(EvalName=="StepsSSS") %>%
  filter(avg_times!=0) %>%
  select(avg_times, success_rate, kernel_reward, vehicle, train_n) %>%
  ggplot(aes(x=train_n, y=success_rate))+
  geom_point()

```


# Archive Tests

## Kernel Reward

There are very interesting results from the evaluations on the kernel reward signal
We start by looking at the direct results from all the tests run.



```{r, fig.cap="Constant Reward Signals"}

test_data %>%
  filter(EvalName=="KernelReward"&test_number==1&kernel_reward=="Constant")%>%
  select(avg_times, success_rate, test_number, reward, kernel_reward, sss_reward_scale, test_number) %>%
  group_by(sss_reward_scale)%>%
  mutate(mean_time=mean(avg_times))%>%
  ggplot(aes(x=sss_reward_scale))+
  geom_boxplot(aes(y=avg_times, group=sss_reward_scale))+
  geom_point(aes(y=avg_times), size=2)+
  geom_line(aes(y=mean_time))+
  geom_point(aes(y=mean_time), shape='x', size=8)

```
The first and most obvious result is that using a reward signal value of zero is produces poor behaviour with slow lap times.
Apart from that, the results show that as the sss reward scale increases the lap times become slower. This is true for the magnitude and constant reward signals.

```{r, fig.cap="Magnitude Reward Signals"}

test_data %>%
  filter(EvalName=="KernelReward"&test_number==1&kernel_reward=="Magnitude")%>%
  select(avg_times, success_rate, test_number, reward, kernel_reward, sss_reward_scale, test_number) %>%
  group_by(sss_reward_scale)%>%
  mutate(mean_time=mean(avg_times))%>%
  mutate(median_time=median(avg_times))%>%
  ggplot(aes(x=sss_reward_scale))+
  geom_boxplot(aes(y=avg_times, group=sss_reward_scale))+
  geom_point(aes(y=avg_times), size=2)+
  geom_line(aes(y=mean_time))+
  geom_point(aes(y=mean_time), shape='x', size=8)
```

What is interesting to note and is not shown on the graphs is that for the 0.2 reward, ti only achieves a success rate of around 97% without the SSS.
This means that it doesn't learn to be totally independant of the supervisor.
This is definitely something to watch out for. 

The interpretation of this result is the clear balance between performance and safety. 



